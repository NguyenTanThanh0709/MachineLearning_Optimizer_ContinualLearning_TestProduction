# MachineLearning_Optimizer_ContinualLearning_TestProduction
# Tác Giả
Nguyễn Tấn Thành
# Giới thiệu
Tôi xin giới thiệu báo cáo của mình về các phương pháp tối ưu trong Deep Learning. Báo cáo tập trung mô tả chi tiết về các phương pháp tối ưu đang được sử dụng rộng rãi trong cộng đồng Deep Learning ngày nay. Bắt đầu từ những lý thuyết và tư tưởng cơ bản của gradient descent, chúng tôi giải thích ưu và nhược điểm của từng phương pháp và đi sâu vào khía cạnh toán học của chúng.

Gradient descent, mặc dù có nhược điểm, vẫn là phương pháp dễ hiểu và phổ biến nhất. Tính đặc trưng của gradient descent là khó vượt qua điểm cực tiểu và khả năng không điều chỉnh learning rate cho từng tham số. Điều này dẫn đến sự xuất hiện của nhiều phương pháp tối ưu sau này, như momentum, NAG, AdaGrad, Adadelta, RMSProp và Adam, nhằm khắc phục nhược điểm của Gradient Descent. Các phương pháp này được thiết kế để tối ưu hóa quá trình huấn luyện mô hình.

Continual Learning (CL) đóng vai trò quan trọng trong lĩnh vực Machine Learning (ML), đòi hỏi khả năng liên tục học từ dữ liệu mới mà không bỏ qua kiến thức đã được học từ dữ liệu cũ. Trong quá trình triển khai giải pháp học máy để giải quyết bài toán, sự đối mặt với dữ liệu mới và thay đổi liên tục là một thực tế phổ biến và quan trọng.

Test Production là quá trình quan trọng, trong đó chúng tôi tạo và triển khai các tập kiểm thử để đánh giá hiệu suất của mô hình.

Chúng tôi cũng đã thực nghiệm việc lập trình lại các thí nghiệm và công bố mã nguồn mở từ các bài báo để cung cấp tài liệu tham khảo cho độc giả. Báo cáo chi tiết có sẵn trong tệp PDF, và mã nguồn có thể được xem trong các tệp Jupyter tương ứng.

